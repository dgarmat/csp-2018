---
title: "Untitled"
author: "Emile Latour"
date: "February 5, 2018"
output: github_document
---

```{r setup, include=FALSE}

#### Global chunk options -----------------------------

knitr::opts_chunk$set(
  eval       = TRUE,    # whether to run code in code chunk
  include    = TRUE,    # whether to include the chunk output
  echo       = TRUE,    # Whether to show code chunk in final output
  error      = TRUE,    # whether to display error messages
  message    = FALSE,   # whether to preserve messages
  warning    = FALSE,   # whether to preserve warnings
  comment    = NA,      # a character string to append at start
                        # of each line of results in final document
  tidy       = FALSE   # whether to tidy code chunks for display
  
)


#### Other options --------------------------------

## Turn off scientific notation ----------------
options(scipen = 999)

## define format ----------------
# If you don't define format here, you'll need put `format = "html"` in 
# every kable function.

options(knitr.table.format = "html")  # use this option when knitting html
# options(knitr.table.format = "latex")  # use this option when knitting pdf

## Set random seed ----------------
seed_for_imp <- 8675309
set.seed(seed_for_imp)


#### Packages --------------------------------------
# This is where we load in all the packages we plan to use

## Define the repository for packages ----------------
options(repos = c(CRAN = "http://cran.rstudio.com"))

# universally useful packages
if (!require("pacman")) install.packages("pacman")
if (!require("devtools")) install.packages("devtools")

# github packages
if (!require("janitor")) devtools::install_github("sfirke/janitor")
if (packageVersion("janitor") < "0.4.0.9000") {
  devtools::install_github("sfirke/janitor")
}
  
# Load the list of packages
pacman::p_load(
  Hmisc,         # contains many functions useful for data analysis
  psych,         # a general purpose toolbox
  tidyverse,     # packages ggplot2, tibble, tidyr, readr, purrr, and dplyr 
  forcats,       # functions for factors, forcats is an anagram for factors
  broom,         # functions tidy(), glance(), augment()
  magrittr,      # includes the %<>% assignment-pipe (%>% is loaded from dplyr)
  rlang,         # For use in tidyevaluation
  readxl,        # read in excel files
  writexl,       # write excel files, zero dependency
  xlsx,          # read, write, format Excel files
  janitor,       # for working with dirty data
  lubridate,     # for working with dates and times
  stringr,       # handy string operations
  tableone,      # Create Table 1 to Describe Baseline Characteristics
  DT,            # render R objects as HTML tables
  knitr,         # A General-Purpose Package for Dynamic Report Generation in R
  kableExtra,    # Enahced table functions
  ggthemes,      # Extra Themes, Scales and Geoms for 'ggplot2'
  scales,        # Scale Functions for Visualization
  visdat,        # Preliminary data visualisation
  naniar,        # structures, summaries, and visualisations for missing data
  here,          # Constructs paths to your project's files
  mice           # Multivariate Imputation by Chained Equations

  )

#### Store time for later --------------------------------

start_time <- Sys.time()

#### Confirm directory --------------------------------

here::here()

```


# Simulate a data set

Due to the sensitive nature of electronic health record (EHR) data, in order to provide a data set for a reproducible example, I wrote code below to simulate a data set that can be used with the rest of the code in this example. The only similarities to the original data are the variable names and the data categories.

Note that the eligibility requirements for screening services are based on sex, age, and medical history. I do not simulate medical history and so the eligility here is based only on sex and age.

The proportions of screenings in the data were chosen to try to simulate some data that would make for interesting example with the rest of the code. The proportions of missingness are similar to the actual work that I presented in that I did a little bit of rounding before simulation.

Disclaimer: All the data in this example is simulated from the following code. Any similarities to the original data set or any other existing data is purely by chance alone.

## Simulate the data

```{r make-fake-data, results=FALSE}

#### Set the variable names --------------------------------

var_names <- c("study_id",
               "sex",
               "age_start",
               "primary_dept",
               "ethnic_cat",
               "lang_cat",
               "race_cat",
               "fpl_cat",
               "age_cat",
               "elig_cervical",
               "elig_breast",
               "elig_colon",
               "elig_colonoscopy",
               "elig_flexsig",
               "elig_fobt",
               "elig_bmi",
               "elig_flu",
               "elig_chlam",
               "elig_smoking",
               "elig_cholest",
               "dmap_cervical",
               "dmap_breast",
               "dmap_colon",
               "dmap_colonoscopy",
               "dmap_flexsig",
               "dmap_fobt",
               "dmap_bmi",
               "dmap_flu",
               "dmap_chlam",
               "dmap_smoking",
               "dmap_cholest",
               "ehr_cervical",
               "ehr_breast",
               "ehr_colon",
               "ehr_colonoscopy",
               "ehr_flexsig",
               "ehr_fobt",
               "ehr_bmi",
               "ehr_flu",
               "ehr_chlam",
               "ehr_smoking",
               "ehr_cholest"
)


#### Make an empty tibble --------------------------------

## Pre-set the number of rows for the data set ----------------

size <- 14000

## Create tibble with NA's ----------------

valdata <- tibble::as.tibble(matrix(NA, nrow = size, ncol = length(var_names)))
names(valdata) <- var_names


#### Make the demographic data --------------------------------

## Function to help make factor variables ----------------

make_factor <- function(levels, size, replace = TRUE, prob = NULL) { 
  
  if (is.null(prob)) { 
    
    factor(sample(levels, size, replace))
    
  } else {
      
    factor(sample(levels, size, replace, prob))
  }
}

## Make the demographic data ----------------
# Note that the proportions for the demographic categories are made up and
# decided upon by me. I intended for them to be similar in magnitude to the
# original study data, but different enough that they could not be compared to 
# or construed to be original values. Variables with percent missing are the 
# same ones as my previous work, but the percent missing is slightly different 
# with rounding.

# 3 group sizes for age groups
a <- floor(size / 3)
b <- a
c <- size - a - b

valdata %<>% 
  mutate(
    study_id = seq(1:size), 
    sex = make_factor(levels = c("F", "M"), 
                      size = size, 
                      prob = c(0.65, 0.35)), 
    age_start = c(sample(c(19:34), size = a, replace = TRUE), 
                  sample(c(35:50), size = b, replace = TRUE), 
                  sample(c(51:64), size = c, replace = TRUE)
    ), 
    age_start = sample(age_start, 
                       size = size, 
                       replace = FALSE), 
    age_cat = cut(age_start, 
                  breaks = c(19, 35, 51, 65), 
                  right = FALSE), 
    primary_dept = make_factor(levels = 
                                 stringr::str_pad(c(1:40), 3, pad = "0"), 
                               size = size), 
    ethnic_cat = make_factor(levels = 
                               c("Hispanic", "NH White", "NH Other", NA), 
                             size = size, 
                             prob = c(0.10, 0.70, 0.15, 0.05)), 
    lang_cat = make_factor(levels = c("English", "Spanish", "Other"), 
                           size = size, 
                           prob = c(0.85, 0.05, 0.10)), 
    race_cat = make_factor(
      levels = c("API", "AIAN", "Black", "White", "Multiple Races", NA), 
      size = size, 
      prob = c(0.05, 0.02, 0.10, 0.75, 0.01, 0.07)), 
    fpl_cat = make_factor(levels = c("<=138% FPL", ">138% FPL", NA), 
                          size = size, 
                          prob = c(0.75, 0.05, 0.20))
  )

# drop variabls no longer needed
rm(a, b, c)


#### Make the eligibility variables --------------------------------
# The original study paper outlined the criteria to be eligible for certain
# screening services. They tended to be based on sex, age, and medical history.
# I will not be generating variables for the relevant medical history for
# eligibility. For the purpose of this simulated data set, I will base
# eligibility only on sex and age.

# 1 = eligible
# 0 = not eligible

# Function to convert to factors 
make_factor2 <- function(var) {
  factor(var, levels = c("1", "0"))
}

valdata %<>% 
  mutate(
    elig_cervical = 
      ifelse(sex == "F" & age_start >= 19 & age_start <= 64, 1, 0), 
    elig_breast = 
      ifelse(sex == "F" & age_start >= 40, 1, 0), 
    elig_colon = 
      ifelse(age_start >= 50, 1, 0), 
    elig_colonoscopy = 
      ifelse(age_start >= 50, 1, 0), 
    elig_flexsig = 
      ifelse(age_start >= 50, 1, 0), 
    elig_fobt = 
      ifelse(age_start >= 50, 1, 0),  
    elig_bmi = 1, 
    elig_flu = 
      ifelse(age_start >= 50, 1, 0), 
    elig_chlam = 
      ifelse(sex == "F" & age_start >= 19 & age_start <= 24, 1, 0), 
    elig_smoking = 1, 
    elig_cholest = 
      ifelse(age_start >= 20, 1, 0)) %>% 
  mutate_at(.vars = vars(elig_cervical:elig_cholest), 
            .funs = funs(make_factor2))


#### Make factor screening variables --------------------------------
# A patient must be elgible to recieve a screening service. Those that are
# eligible based on the step above will be randomly assigned 1/0 (screened/not
# screened); those that are not elgible will be assigned 0 (not screened).

## Function to help make the screening data ----------------
# Split the data set into eligible and not eligible. Randomly assign
# screen/not-screened to those that are eligible. Not screened to those that are
# not elgibile. Combine the split data sets.

# Returns a data frame / tibble.

make_screening <- function(df, 
                           elig_var, 
                           dmp_scr, 
                           ehr_scr, 
                           dmp_prob, 
                           ehr_prob) { 
  
  require(rlang)
  require(dplyr)
  
  elig_enq <- enquo(elig_var)
  dmp_enq <- enquo(dmp_scr)
  dmp_name <- quo_name(dmp_enq)
  ehr_enq <- enquo(ehr_scr)
  ehr_name <- quo_name(ehr_enq)
  
  num_elig <- df %>% 
    dplyr::filter(!! elig_enq == 1) %>% 
    dplyr::count(!! elig_enq) %>% 
    dplyr::pull()
  
  num_not_elig <- df %>% 
    dplyr::filter(!! elig_enq != 1) %>% 
    dplyr::count(!! elig_enq) %>% 
    dplyr::pull()
  
  df_elig <- df %>% 
    dplyr::filter(!! elig_enq == 1) %>% 
    mutate(
      !! dmp_name := rbinom(n = num_elig, size = 1, prob = dmp_prob), 
      !! ehr_name := rbinom(n = num_elig, size = 1, prob = ehr_prob), 
      !! dmp_name := factor(!! dmp_enq, levels = c(1, 0)), 
      !! ehr_name := factor(!! ehr_enq, levels = c(1, 0))
    )
  
  df_not_elig <- df %>% 
    dplyr::filter(!! elig_enq != 1) %>% 
    mutate(
      !! dmp_name := 0, 
      !! ehr_name := 0, 
      !! dmp_name := factor(!! dmp_enq, levels = c(1, 0)), 
      !! ehr_name := factor(!! ehr_enq, levels = c(1, 0))
    )
    
  df_elig %>% 
    dplyr::bind_rows(., df_not_elig) %>% 
    dplyr::arrange(., study_id)

}

## Make the screenings values ----------------

valdata <- make_screening(df = valdata, 
                          elig_var = elig_cervical, 
                          dmp_scr = dmap_cervical, 
                          ehr_scr = ehr_cervical, 
                          dmp_prob = 0.30, 
                          ehr_prob = 0.30)

valdata <- make_screening(df = valdata, 
                          elig_var = elig_breast, 
                          dmp_scr = dmap_breast, 
                          ehr_scr = ehr_breast, 
                          dmp_prob = 0.40, 
                          ehr_prob = 0.40)

valdata <- make_screening(df = valdata, 
                          elig_var = elig_colon, 
                          dmp_scr = dmap_colon, 
                          ehr_scr = ehr_colon, 
                          dmp_prob = 0.30, 
                          ehr_prob = 0.30)

valdata <- make_screening(df = valdata, 
                          elig_var = elig_colonoscopy, 
                          dmp_scr = dmap_colonoscopy, 
                          ehr_scr = ehr_colonoscopy, 
                          dmp_prob = 0.10, 
                          ehr_prob = 0.10)

valdata <- make_screening(df = valdata, 
                          elig_var = elig_flexsig, 
                          dmp_scr = dmap_flexsig, 
                          ehr_scr = ehr_flexsig, 
                          dmp_prob = 0.05, 
                          ehr_prob = 0.05)

valdata <- make_screening(df = valdata, 
                          elig_var = elig_fobt, 
                          dmp_scr = dmap_fobt, 
                          ehr_scr = ehr_fobt, 
                          dmp_prob = 0.15, 
                          ehr_prob = 0.30)

valdata <- make_screening(df = valdata, 
                          elig_var = elig_bmi, 
                          dmp_scr = dmap_bmi, 
                          ehr_scr = ehr_bmi, 
                          dmp_prob = 0.05, 
                          ehr_prob = 0.85)

valdata <- make_screening(df = valdata, 
                          elig_var = elig_flu, 
                          dmp_scr = dmap_flu, 
                          ehr_scr = ehr_flu, 
                          dmp_prob = 0.35, 
                          ehr_prob = 0.40)

valdata <- make_screening(df = valdata, 
                          elig_var = elig_chlam, 
                          dmp_scr = dmap_chlam, 
                          ehr_scr = ehr_chlam, 
                          dmp_prob = 0.50, 
                          ehr_prob = 0.40)

valdata <- make_screening(df = valdata, 
                          elig_var = elig_smoking, 
                          dmp_scr = dmap_smoking, 
                          ehr_scr = ehr_smoking, 
                          dmp_prob = 0.05, 
                          ehr_prob = 0.95)

valdata <- make_screening(df = valdata, 
                          elig_var = elig_cholest, 
                          dmp_scr = dmap_cholest, 
                          ehr_scr = ehr_cholest, 
                          dmp_prob = 0.40, 
                          ehr_prob = 0.40)


#### Remove what's no longer needed --------------------------------

rm(make_factor, make_factor2, make_screening, size, var_names)

```

## Do some checks and take a glimpse

```{r}
dplyr::glimpse(valdata)
```


TODO -- Change the name of the data set everywhere else.
TODO -- test the user functions


# EDA / explore missingness

## Table One

A quick table one to look at the demographics of the sample data.

```{r}
#### Create a table one --------------------------------
# Table one summary stats using the tableone package

tab1 <- tableone::CreateTableOne(
  vars = c("sex", "race_cat", "ethnic_cat", "lang_cat", "fpl_cat", "age_cat"), 
  data = valdata, 
  factorVars = 
    c("sex", "race_cat", "ethnic_cat", "lang_cat", "fpl_cat", "age_cat"), 
  includeNA = TRUE
)

#### print_table_one --------------------------------
# A helper function to print the table one object to my preference.

print_table_one <- . %>% 
  print(., 
        showAllLevels = TRUE, 
        printToggle = FALSE, 
        noSpaces = TRUE
        ) %>% 
  as.data.frame(.) %>% 
  tibble::rownames_to_column(., var = "rowname") %>% 
  knitr::kable(format = "html", 
               booktabs = TRUE, 
               longtable = TRUE, 
               col.names = c("", names(.)[-1])) %>% 
  kableExtra::kable_styling(full_width = FALSE, 
                            latex_options = c("repeat_header")) %>% 
  kableExtra::column_spec(1, width = "10em")

#### Print table one --------------------------------

tab1 %>% 
  print_table_one
```

## Visualizing missingness in the data set

The table one above shows that there are missing data in the variables `race_cat`, `ethnic_cat`, and `fpl_cat`. When beginning to think about how to handle the missing data in your project, visualization is a great place to begin. The [`naniar`](https://cran.r-project.org/web/packages/naniar/index.html) and the [`visdat`](https://cran.r-project.org/web/packages/visdat/index.html) packages provide helpful plots.

### The `vis_miss()` plot

In the figure below, we get an overview of the missing values in the data set. Missing are shown in black and observed values are shown in gray. We see that there are only 3 variables in the data set with missing values.

```{r}
naniar::vis_miss(valdata)
```

We can also get a numeric percent missing for the variables.

```{r}
miss_var_summary(valdata) %>% 
  head(.)
```

### Setting `cluster = TRUE`

It can also be useful to look at the missingness plot with the values clustered. This gives a sense of how many values are missing in one row. Many rows with multiple missing values can be problematic when trying to do impuation.

I'm only going to show this plot for a few variables since we saw above that many were compeletely observed.

```{r}

valdata %>% 
  dplyr::select(race_cat, ethnic_cat, fpl_cat, lang_cat, sex, age_cat) %>% 
  naniar::vis_miss(., 
                   cluster = TRUE)

```

### The `VIM`package

The [`VIM`](https://cran.r-project.org/web/packages/VIM/VIM.pdf) package also has many helpful tools for visualizing missing values. I really like their combination bar plot and aggregation plot. 

Here I have filtered the data to just those with missing values; I think that this helps the plot to be more clear. The barplot on the left shows the proportion of missing values in each variable. The aggregation plot on the right shows the combinations of missing (dark gray) and observed (light gray) values that exist in the data.

```{r}
valdata %>% 
  dplyr::select(race_cat, ethnic_cat, fpl_cat) %>% 
  VIM::aggr(., 
            col = c("gray", "gray29"), 
            numbers = TRUE, 
            sortVars = TRUE, 
            labels = names(.), 
            cex.axis = .7, 
            gap = 3, 
            ylab = c("Histogram of missing data","Pattern"))
```


## Numerical summaries

```{r}
mssng_pattern <- valdata %>% 
  dplyr::select(race_cat, ethnic_cat, fpl_cat) %>% 
  mice::md.pattern(.) %>% 
  as.data.frame() %>% 
  tibble::as_tibble(.) %>% 
  tibble::rownames_to_column(., var = "count") %>% 
  dplyr::rename(., 
                # new = old
                num_available = V4)

mssng_pattern %>% 
  kable(.)
```

See Stef Van Buuren's vignette for more in depth interpretation of the tabe above. There's a lot of good info there. How I interpret it is to pay attention to the 1's and 0's in the main body of the table. They correspond to what is observed (1) and missing (0) for the variables listed in the top row.

So the first row of all 1's means that all listed variables are observed; the count to the left shows the number of rows in the data set that fit this description. The last row of all 0's means that all variables are missing; similar that the count on the left shows the number of rows in the data set that are missing for all the variables shown. Then all the 1's and 0's in between represent some combination of missingness among the variables.

The `naniar` package has summary that is a little simpler and gives some of the same information. But not as much detail as the `md.pattern()` in the `mice` package.

```{r}
valdata %>% 
  naniar::miss_case_table(.) %>% 
  kable(.)
```


### Number of observations per pattern of missing pairs

```{r}
mssng_pairs <- valdata %>% 
  dplyr::select(ethnic_cat, race_cat, fpl_cat) %>% 
  mice::md.pairs(.)

mssng_pairs
```

Four missingness patterns:

+ `rr` both are observed,
+ `rm` first variable is observed, the second is missing,
+ `mr` first variable is missing, the second is observed, and
+ `mm` both variable are missing.

### Proportion of usable cases

Measures how many cases with missing data on the target variable actually have observed values on the predictor. The proportion will be low if both target and predictor are missing on the same cases.

```{r}

prop_usable_cases <- valdata %>% 
  dplyr::select(ethnic_cat, race_cat, fpl_cat) %>% 
  mice::md.pairs(.)

with(prop_usable_cases, 
     round(mr / (mr + mm), digits = 3))
```

Target on the vertical axis (i.e. left), predictor on the horizontal (i.e. top).

Interpret: Of the records with values for `ethnic_cat`, xx% have observed information on `race_cat` and xx% have observed information on `fplp_cat`. Etc.

This gives a sense of what variables may be good to include/exlcude in the imputation model. Higher % indicates more informaton and likely good predictor; lower % indicates that the variables are missing for the same observations and may not be good predictor.

### Number of incomplete cases

More recent advise on how many imputations to perform suggests a rule of thumb that the number of imputations should be similar to the percentage of cases that are incomplete. So, not just as a part of the EDA, it is important to know the number of incomplete cases to inform the specification of the imputation model later.

An incomplete case would be an obervation (or row) with at least one missing value. There are a number of ways to get at this information, but the `naniar` package makes it super easy.

```{r}

n_missing <- naniar::miss_case_prop(valdata)
n_missing

```

So we know that in the data set `r scales::percent(n_missing)` of observations have missing values. I round this up to `r max(pretty(100 * n_missing))` to select the number of imputations to perform.


# Imputation

Below I show some of the set up that I went through to perform the multiple imputations using the `mice` package in R. Some resources were abosolutely indespensible in my set up and learning: 

+ the [`mice` package documentation](https://cran.r-project.org/web/packages/mice/mice.pdf)
+ the [vignette](https://www.jstatsoft.org/article/view/v045i03/v45i03.pdf) from the Journal of Statistical Software (December 2011, Volume 45, Issue 3)
+ [Flexible Imputation of Missing Data](https://www.crcpress.com/Flexible-Imputation-of-Missing-Data/van-Buuren/p/book/9781439868249) by Stef van Buuren

Also, since my work was done, [online resources](https://cran.r-project.org/web/packages/mice/vignettes/resources.html) have been added.

For more information about MICE: Multivariate Imputation by Chained Equations, sometimes called Fully Conditional Specification, I highly recommend any of the materials (code or books) or published papers by Stef van Buuren. Much can be found through his website, [www.multiple-imputation.com](http://www.stefvanbuuren.nl/mi/).

## Some set up

Here I am just going to define the number of imputations and the number of iterations to objects for use in later code. This is just a convenience step so that I only have to update these values in one place if I want to change them later.

The default number of impuations in the `mice` software is 5. Based on the exploration of the missingness above, we saw that they number of imputations suggested is much higher. Here in this example, I am going to keep it set at the default 5, just to limit the compuation time if someone want to run this code on their own.

In practice, my suggestion would be to "tune" the imputation using a lower number like the default setting. Then once the impuation model is set, perform you final or near final impuations using the higher number as suggested by recent literature.

As far as iterations go, they tend to converge rather quickly with the `mice` algorithm. My advice is to run them out long enough to see whether there is convergence or not, while not getting super bogged down with computation time.

Here I am going to continue to use the software default of 5 for our example. In my actual work, I used 20. In your practice I would suggest to try to use 10 to 20.

```{r}
imp_num <- 5  # number of imputations, dflt = 5
iter_num <- 5  # number of interations, dflt = 5

```

We can also tel the `mice` software to run an "initial" empty impuation. The only effect of this is to give us some objects in R to work with as we go through the steps. See below where I run the initial impuation. Note that the maximum number of iterations (`maxit`) is set to zero.

```{r}
init <- mice::mice(valdata, maxit = 0)
meth <- init$method
predM <- init$predictorMatrix
# print(ini)

```

The `init` object contains lots of information that we will work with as we go forward. Of interest for now though is the method selected for the variables. This is the form of the imputation model for the variables to be imputed.

```{r}
meth
```

We see that the software made no choice for the variables without missing data. For those with missing data, based on the type of variable, it makes some default choices. We can override these later.

We also get the default matrix of predictors that the sofware chose. This is an object of 1's and 0's. For those variables with no missing and that we won't be imputing, the values are zero. Here is a glimpse of just those that we intend to impute.

```{r}
predM[rowSums(predM) > 0, ]
```



## Specify the imputation model

Here I will follow the 7 steps that van Buuren suggests in order to set up the algorithm. See his writings for more details than I will go into here.

### Step 1 - Decide if the missing at random (MAR) assumption is reasonable

In this example, we randomly assigned missing values. So here, it kind of has to be reasonable. In practice thoug, this can be challenging to know for sure which is why the exploration of the data and the missingness is such an important step to take as I showed above.

Assuming MAR is typically a reasonable place to start. There is literature on sensitivity analysis with the imputations to examine if this assumption is met. And there are techniques to model the missing mechanism with the imputation if there is violation. This work is outside the scope of what I hope to share here.

### Step 2 - Decide on the form of the imputation model

We want to decide the form of the model used to impute the missing values of each variable. This can be specified on a variable by variable basis. We saw above from the `meth` object that the software made default decisions for us. 

__FPL__ -- logistic regression (`logreg`), for factor with 2 levels.

__Race__ -- Multinomial logit regression (`polyreg`), factor with > 2 levels.

__Ethnicity__ -- Multinomial logit regression (`polyreg`), factor with > 2 levels.

I am going to overwrite those just to show how it is done. By overwriting the meth object we can force the algorithm to use this later.

```{r}
meth[c("ethnic_cat")] <- "polyreg"
meth[c("race_cat")] <- "polyreg"
meth[c("fpl_cat")] <- "logreg"
meth
```

### Step 3 - Decide the set of predictors to include in the imputation model

What variables to include in the multiple imputation model? 

The advice is to include as many relevant variables as possible. One should include all variables that are in your scientific model of interest that will be used after imputation. Also variables that are related to the missingness of the variables you are imputing. Van Buuren has more advice here.

Including as many predictors as possible makes the MAR assumption more reasonable. But with larger data sets, this is not advisable for computation purposes. Van Buuren suggests that 15 to 25 variables will work well. He also offers advice to cull that list.

My case is interesting. I am not doing modelling; I am calculating scalar statistics of agreement. Also, my data set isn't really too large (41 variables once you ignore study ID which isn't too important for imputation purposes). 

To aid in these decisions the `mice` package has a function that produces a "quick predictor matrix" that is useful for dealing with data sets with large number of variables. The software chooses by calculating two correlations with the avaialable cases, taking the larger, and seeing if it meets a minimum theshhold. See `?quickpred` for better description.

Below I run the `quickpred()` to see what the software chooses. Only show the matrix below for those records with > 1 rows or columns

```{r}
predGuess <- valdata %>% 
  mice::quickpred(.)

predGuess[rowSums(predGuess) > 0, colSums(predGuess) > 0]

```

Hmmm. As I am working through this example with the simulated data, the software did not choose any. In my actual work, it discovered about 2 to 3 important predictors for each variable.

In my actual work, I went to the lead investigator for insight into the data set and suggestions on which variables would be informative. The code chunk below shows how I took the list that they provided and modified the `predM` oject.

```{r}
# Store the names of the variables in an object
var_names <- dput(names(valdata))

# create another vector of the names selected by the investigator
pi_list <-
  c("fpl_cat", "race_cat", "ethnic_cat", "lang_cat", "age_start", "sex", 
    "primary_dept", "ehr_cervical", "ehr_breast", "ehr_colon", 
    "ehr_colonoscopy", "dmap_breast", "dmap_colonoscopy", "ehr_cholest", 
    "dmap_cholest", "elig_cholest", "ehr_flexsig", "ehr_fobt", "ehr_bmi", 
    "ehr_flu", "ehr_chlam", "ehr_smoking", "dmap_cervical", "dmap_colon", 
    "dmap_flexsig", "dmap_fobt", "dmap_bmi", "dmap_flu", "dmap_chlam", 
    "elig_cervical", "elig_breast", "elig_colon", "elig_bmi", "elig_flu", 
    "elig_chlam", "elig_smoking")



```

Note that the investigator had me exclude the variables, `elig_colonoscopy`, `elig_flexsig`, and `elig_fobt`, because these have the exact same information as `elig_colon`. The eligibility for all these screenings is the same.

We also did not include `dmap_smoking` because there was little to no information here.

Also, we included `age_start` as a continuous variable and did not include the categorical version of this variable, `age_cat`, hoping to get more information.


```{r}
# Make a vaector of the variable names that we want to include.
vars_to_include <- var_names[(var_names %in% pi_list)]

# varNames[!(varNames %in% JH_list)]

pred <- predGuess
pred[, ] <- 0
pred[, vars_to_include] <- 1
diag(pred) <- 0
# head(pred)
dim(pred)
head(pred)
```




# Make the visiting scheme

Pretty sure that this doesn't do anything.

```{r}
miOrder <- vars_to_include %>% 
  match(., JH_list) %>% 
  as.numeric(.)

comboList <- data.frame(vars_to_include, JH_list) %>% 
  cbind(., miOrder)
  
comboList %>% 
  kable(.)

```



## Visting sequence

This tells `mice` the order to impute the missing variables. I looked at high to low and low to high. Didn't seem to make a difference. van Buuren says that it would only really matter with longitudinal studies.

`init$visitSequence` give the varaibles to be imputed and their column positions.

```{r}
init <- mice::mice(cardiac, maxit = 0)
meth <- init$method
predM <- init$predictorMatrix
# print(ini)
 

init$visitSequence
# ethnic.cat   race.cat    fpl.cat 
#          5          7          8 
```


### Define the order for visiting

FPL -> Race -> Ethnicity

```{r}
visit_order <- c(init$visitSequence[["fpl.cat"]], 
                 init$visitSequence[["race.cat"]], 
                 init$visitSequence[["ethnic.cat"]])
```


## Predictor matrices



### Full list 

```{r}
pred <- predM
pred[, ] <- 0
pred[, vars_to_include] <- 1
diag(pred) <- 0

head(pred[, 1:10], n = 10)

```

# Imputations (FPL, Race, and Ethnicity)

## Full list (41 vars)

```{r, cache=FALSE}
system.time(
imp_full <- 
  mice::mice(data = cardiac, 
             m = imp_num,  # number of imputations, dflt = 5
             method = meth,  # specify the method
             predictorMatrix = pred, 
             visitSequence = visit_order, 
             seed = seed_for_imp, 
             maxit = iter_num,  # number of interations, dflt = 5
             print = FALSE
             )
)
 #   user  system elapsed 
 # 753.92   10.39  765.00 

# with 20 imputations
#    user  system elapsed 
# 3782.48   25.14 3821.28
```

### Plot of convergence

```{r}
plot(imp_full, c("ethnic.cat", "race.cat", "fpl.cat"))
```


# Look at the imputed data sets

```{r}

mylist <- list() #create an empty list

for (i in 1:imp_full$m) {
  
  mylist[[i]] <- mice::complete(imp_full, i)
  
}

mylist[[1]]


output <- tibble::enframe(mylist, name = "imp", value = "data") %>% 
  tidyr::unnest()

# This would do the same thing, maybe faster, but the above has the added 
# benefit of easily adding the number of the imputation so that I can track 
# things. Either way, I chose the one above as my preference.
# df <- do.call(rbind, mylist)
# df

# Should be 393,030 rows = 30 sets X 13101 rows per set
dplyr::glimpse(output)
head(output)
tail(output)

```


```{r}
make_cardiac_long <- function(df) { 
  
  # Reshape ELIG, DMAP, and EHR
  df_long <- df %>% 
    tidyr::gather(data = ., 
                  key = "vars", 
                  value = "value", 
                  ELIG_cervical:EHR_cholest) %>% 
    tidyr::separate(col = vars, 
                    into = c("EHR_DMAP_ELIG", "proc"), 
                    sep = "_", 
                    remove = TRUE) %>% 
    dplyr::mutate(proc = case_when(
      proc == "BMI" ~ "bmi", 
      proc == "FlexSig" ~ "flexsig", 
      proc == "FOBT" ~ "fobt", 
      proc == "Weight" ~ "weight", 
      TRUE ~ proc
    )) %>% 
    tidyr::spread(data = ., 
                  key = EHR_DMAP_ELIG, 
                  value = value)
  
  # Gather the categories
  df_long %<>% 
    tidyr::gather(data = ., 
                  key = "cat", 
                  value = "level", 
                  sex, ethnic.cat:age.cat) %>% 
    mutate(cat = case_when(
      cat == "age.cat" ~ "age", 
      cat == "ethnic.cat" ~ "ethnicity", 
      cat == "fpl.cat" ~ "fpl", 
      cat == "lang.cat" ~ "language", 
      cat == "race.cat" ~ "race", 
      TRUE ~ cat
    ))
  
  
  # Pick one category and dummy the category and the levels
  # And bind the all back to the long data
  df_long %<>% 
    dplyr::filter(cat == "sex") %>% 
    mutate(cat = "all", 
           level = "all") %>% 
    dplyr::bind_rows(., df_long)
  
  df_long %>%
    dplyr::select(imp, 
                  StudyID,
                  age_start,
                  PrimaryDept,
                  proc, 
                  cat,
                  level, 
                  ELIG, 
                  EHR, 
                  DMAP
                  )
  
  }
```


```{r}


df <- make_cardiac_long(df = output)

df

df %>% 
  group_by(cat) %>% 
  summarise(
    n = n(), 
    ELIG = sum(as.numeric(ELIG), na.rm = TRUE), 
    EHR = sum(as.numeric(EHR), na.rm = TRUE), 
    DMAP = sum(as.numeric(DMAP), na.rm = TRUE), 
    id = sum(as.numeric(StudyID), na.rm = TRUE), 
    age = sum(age_start, na.rm = TRUE)
  )



```


```{r}
df_nested <- df %>% 
  group_by(proc, cat, level, imp) %>% 
  nest()



df_nested
df_nested$data[[1]]
head(df_nested$data[[1]])
tail(df_nested$data[[1]])



```



```{r, warning=FALSE}

foo <- df_nested %>% 
  mutate(Q = purrr::map(.x = data, .f = calc_stats_p), 
         U = purrr::map(.x = data, .f = calc_stats_se))

foo
foo$data[[1]]

bar <- foo %>% 
  tidyr::unnest(Q, U) %>% 
  dplyr::rename(Q = Q, 
                U = Q1) %>% 
  dplyr::select(-stat1) %>% 
  dplyr::filter(! (stat %in% c("EHR.t", "DMAP.t", "Combo.t"))) %>% 
  mutate_at(.vars = vars(Q, U), .funs = funs(as.numeric))


bar <- bar %>% 
  group_by(proc, cat, level, stat) %>% 
  nest()

bar$data[[1]]

bar <- bar %>% 
  mutate(pooled = purrr::map(.x = data, .f = mi_pool))

baz <- bar %>% 
  unnest(pooled)


```

```{r}

baz %>% 
  dplyr::select(proc:stat, qbar) %>% 
  tidyr::spread(., key = stat, value = qbar)

```

```{r}

baz %>% 
  group_by(proc, cat, level) %>% 
  count()


12 * 19


length(unique(baz$level))
length(unique(baz$cat))
length(unique(baz$proc))

```

